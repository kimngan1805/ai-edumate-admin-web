# app/web_searching/content_scraper.py
"""
Two-Stage Intelligent Content Scraper
Stage 1: Fast pre-filtering (basic checks)
Stage 2: AI quality analysis (Gemini)
"""

import os
import time
import threading
from pathlib import Path
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import trafilatura
import re
import json


class TwoStageContentScraper:
    """Two-stage content scraper v·ªõi pre-filtering v√† AI analysis"""
    
    def __init__(self, base_path="data", gemini_api_key=None):
        self.base_path = Path(base_path)
        self.setup_directories()
        self.setup_session()
        self.processed_links = set()
        self.rejected_links = set()
        self.prefilter_rejected = set()  # Links b·ªã lo·∫°i ·ªü stage 1
        self.running = False
        self.thread = None
        
        # AI Configuration
        self.gemini_api_key = gemini_api_key or "AIzaSyAxyew-YwI4QfOzFHJQhGSaG0T1uMj6ALo"
        self.setup_gemini()
        
        # Paths
        self.detail_links_base = self.base_path / "scraped_data" / "detail_link"
        self.single_detail_file = self.detail_links_base / "single_detail_link.txt"
        self.multi_detail_links_path = self.detail_links_base / "multi_detail_link"
        self.output_path = self.base_path / "links_md"
        self.processed_links_file = self.output_path / "processed_links.txt"
        self.rejected_links_file = self.output_path / "rejected_links.txt"
        self.prefilter_rejected_file = self.output_path / "prefilter_rejected.txt"
        self.quality_log_file = self.output_path / "quality_analysis.json"
        
        # Load processed/rejected links
        self.load_processed_links()
        self.load_rejected_links()
        self.load_prefilter_rejected()
        
        # Quality thresholds
        self.min_content_length = 300
        self.min_educational_score = 7
        
        # Pre-filter configuration
        self.prefilter_config = {
            'min_raw_content_length': 150,  # T·ªëi thi·ªÉu 150 k√Ω t·ª± raw
            'max_link_density': 0.6,        # T·ªëi ƒëa 60% l√† links
            'min_text_lines': 5,            # T·ªëi thi·ªÉu 5 d√≤ng text
            'blocked_patterns': [
                'ch·ªâ c√≥ link', 'only links', 'danh s√°ch link',
                'download file', 't·∫£i file', 'file ƒë√≠nh k√®m',
                'li√™n h·ªá mua', 'thanh to√°n', 'g·ª≠i ph√≠',
                'ƒëƒÉng nh·∫≠p ƒë·ªÉ xem', 'ƒëƒÉng k√Ω ƒë·ªÉ t·∫£i'
            ],
            'required_educational_keywords': [
                'b√†i t·∫≠p', 'ƒë·ªÅ thi', 'l·ªùi gi·∫£i', 'ph√¢n t√≠ch',
                'l√Ω thuy·∫øt', 'c√¥ng th·ª©c', 'v√≠ d·ª•', 'gi·∫£i th√≠ch',
                'so·∫°n b√†i', 't√≥m t·∫Øt', 'n·ªôi dung', 'ch∆∞∆°ng',
                'h·ªçc sinh', 'gi√°o vi√™n', 'm√¥n h·ªçc'
            ]
        }
        
    def setup_directories(self):
        """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
        directories = [
            self.base_path / "links_md",
            self.base_path / "scraped_data" / "detail_link",
            self.base_path / "scraped_data" / "detail_link" / "multi_detail_link"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
    def setup_session(self):
        """Setup session v·ªõi headers"""
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        self.session.headers.update(self.headers)
    
    def setup_gemini(self):
        """Setup Gemini AI"""
        self.gemini_model = None
        if self.gemini_api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.gemini_api_key)
                
                available_models = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-1.0-pro']
                for model_name in available_models:
                    try:
                        self.gemini_model = genai.GenerativeModel(
                            model_name,
                            generation_config={
                                'temperature': 0.1,
                                'top_p': 0.8,
                                'max_output_tokens': 1024,  # type: ignore # Gi·∫£m ƒë·ªÉ ti·∫øt ki·ªám
                            }
                        )
                        print(f"‚úÖ Gemini AI initialized with model: {model_name}")
                        break
                    except Exception:
                        continue
                        
                if not self.gemini_model:
                    print("‚ùå No Gemini model available")
                    
            except ImportError:
                print("‚ö†Ô∏è google-generativeai not installed. AI quality check disabled.")
        else:
            print("‚ö†Ô∏è No Gemini API key provided. AI quality check disabled.")
    
    def load_processed_links(self):
        """Load danh s√°ch links ƒë√£ x·ª≠ l√Ω"""
        if self.processed_links_file.exists():
            try:
                with open(self.processed_links_file, 'r', encoding='utf-8') as f:
                    self.processed_links = set(line.strip() for line in f if line.strip())
                print(f"üìã Loaded {len(self.processed_links)} processed links")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading processed links: {e}")
                self.processed_links = set()
        else:
            self.processed_links = set()
    
    def load_rejected_links(self):
        """Load danh s√°ch links b·ªã t·ª´ ch·ªëi ·ªü stage 2"""
        if self.rejected_links_file.exists():
            try:
                with open(self.rejected_links_file, 'r', encoding='utf-8') as f:
                    self.rejected_links = set(line.strip().split(' | ')[0] for line in f if line.strip())
                print(f"üö´ Loaded {len(self.rejected_links)} AI-rejected links")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading rejected links: {e}")
                self.rejected_links = set()
        else:
            self.rejected_links = set()
    
    def load_prefilter_rejected(self):
        """Load danh s√°ch links b·ªã t·ª´ ch·ªëi ·ªü stage 1"""
        if self.prefilter_rejected_file.exists():
            try:
                with open(self.prefilter_rejected_file, 'r', encoding='utf-8') as f:
                    self.prefilter_rejected = set(line.strip().split(' | ')[0] for line in f if line.strip())
                print(f"üîç Loaded {len(self.prefilter_rejected)} pre-filter rejected links")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading prefilter rejected links: {e}")
                self.prefilter_rejected = set()
        else:
            self.prefilter_rejected = set()
    
    def save_prefilter_rejected(self, url, reason=""):
        """L∆∞u link b·ªã t·ª´ ch·ªëi ·ªü stage 1"""
        self.prefilter_rejected.add(url)
        try:
            with open(self.prefilter_rejected_file, 'a', encoding='utf-8') as f:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                f.write(f"{url} | {reason} | {timestamp}\n")
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving prefilter rejected link: {e}")
    
    def get_page_content_lightweight(self, url):
        """L·∫•y n·ªôi dung nhanh cho pre-filtering"""
        try:
            response = self.session.get(url, timeout=15)  # Timeout ng·∫Øn h∆°n
            response.raise_for_status()
            
            if response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                response.encoding = 'utf-8'
            
            return response.text
        except Exception as e:
            print(f"   ‚ùå Pre-filter load error: {e}")
            return None
    
    def extract_content_fast(self, html_content, url):
        """Extract n·ªôi dung nhanh cho pre-filtering"""
        if not html_content:
            return None
        
        try:
            # Quick extraction with trafilatura
            extracted = trafilatura.extract(
                html_content,
                output_format="text",  # Plain text ƒë·ªÉ nhanh h∆°n
                include_links=False,
                favor_precision=True,  # ∆Øu ti√™n t·ªëc ƒë·ªô
                url=url
            )
            
            if extracted:
                return extracted.strip()
                
            # Fallback to simple BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove scripts, styles
            for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                tag.decompose()
            
            # Get text from main content areas
            main_selectors = ['article', '.content', '.post-content', '.entry-content', 'main']
            for selector in main_selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            # Fallback to body
            body = soup.find('body')
            if body:
                return body.get_text(strip=True)
            
            return None
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Fast extract error: {e}")
            return None
    
    def prefilter_analysis(self, content, url):
        """Stage 1: Pre-filtering analysis (nhanh, kh√¥ng d√πng AI)"""
        if not content:
            return {
                'passed': False,
                'reason': 'No content extracted',
                'score': 0
            }
        
        content_lower = content.lower()
        content_length = len(content)
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        
        # Check 1: Minimum content length
        if content_length < self.prefilter_config['min_raw_content_length']:
            return {
                'passed': False,
                'reason': f'Too short: {content_length} chars',
                'score': 1
            }
        
        # Check 2: Minimum text lines
        if len(lines) < self.prefilter_config['min_text_lines']:
            return {
                'passed': False,
                'reason': f'Too few lines: {len(lines)}',
                'score': 1
            }
        
        # Check 3: Blocked patterns
        for pattern in self.prefilter_config['blocked_patterns']:
            if pattern in content_lower:
                return {
                    'passed': False,
                    'reason': f'Contains blocked pattern: {pattern}',
                    'score': 2
                }
        
        # Check 4: Link density (approximation)
        link_indicators = content_lower.count('http') + content_lower.count('www.') + content_lower.count('.html')
        text_words = len(content.split())
        link_density = link_indicators / max(text_words, 1)
        
        if link_density > self.prefilter_config['max_link_density']:
            return {
                'passed': False,
                'reason': f'High link density: {link_density:.2f}',
                'score': 2
            }
        
        # Check 5: Educational keywords
        educational_keyword_count = sum(1 for keyword in self.prefilter_config['required_educational_keywords'] 
                                      if keyword in content_lower)
        
        if educational_keyword_count < 2:  # C·∫ßn √≠t nh·∫•t 2 t·ª´ kh√≥a gi√°o d·ª•c
            return {
                'passed': False,
                'reason': f'Too few educational keywords: {educational_keyword_count}',
                'score': 3
            }
        
        # Check 6: Simple answer patterns
        simple_answer_patterns = ['ƒë√°p √°n:', 'answer:', 'a)', 'b)', 'c)', 'd)']
        simple_answers = sum(1 for pattern in simple_answer_patterns if pattern in content_lower)
        
        if simple_answers > len(lines) * 0.5:  # H∆°n 50% l√† ƒë√°p √°n ƒë∆°n gi·∫£n
            return {
                'passed': False,
                'reason': f'Mostly simple answers: {simple_answers}/{len(lines)}',
                'score': 3
            }
        
        # Calculate pre-filter score
        score = 5  # Base score
        
        # Bonus points
        if content_length > 500:
            score += 1
        if educational_keyword_count >= 5:
            score += 1
        if len(lines) >= 10:
            score += 1
        
        return {
            'passed': True,
            'reason': f'Pre-filter passed: {educational_keyword_count} edu keywords, {content_length} chars',
            'score': min(score, 8)  # Max 8 for pre-filter
        }
    
    def stage1_prefilter(self, url):
        """Stage 1: Pre-filtering m·ªôt URL"""
        print(f"   üîç STAGE 1: Pre-filtering...")
        
        # Get content quickly
        html_content = self.get_page_content_lightweight(url)
        if not html_content:
            self.save_prefilter_rejected(url, "Failed to load page")
            return None
        
        # Extract content fast
        raw_content = self.extract_content_fast(html_content, url)
        if not raw_content:
            self.save_prefilter_rejected(url, "No content extracted")
            return None
        
        # Pre-filter analysis
        prefilter_result = self.prefilter_analysis(raw_content, url)
        
        print(f"   üìä Pre-filter score: {prefilter_result['score']}/8")
        print(f"   üìù Reason: {prefilter_result['reason']}")
        
        if not prefilter_result['passed']:
            print(f"   üö´ STAGE 1 REJECTED: {prefilter_result['reason']}")
            self.save_prefilter_rejected(url, prefilter_result['reason'])
            return None
        
        print(f"   ‚úÖ STAGE 1 PASSED: Ready for AI analysis")
        return {
            'raw_content': raw_content,
            'prefilter_score': prefilter_result['score'],
            'prefilter_reason': prefilter_result['reason']
        }
    
    def stage2_ai_analysis(self, content, url):
        """Stage 2: AI analysis v·ªõi Gemini"""
        if not self.gemini_model:
            return {
                'is_quality': True,  # Default pass n·∫øu kh√¥ng c√≥ AI
                'score': 6,
                'reason': 'AI not available - passed pre-filter',
                'content_type': 'unknown'
            }
        
        try:
            print(f"   ü§ñ STAGE 2: AI analysis...")
            
            # Compact prompt to save tokens
            prompt = f"""Analyze this educational content. Return JSON only:
{{
    "is_quality": true/false,
    "score": 1-10,
    "reason": "brief reason",
    "content_type": "full_lesson|partial_content|test_answers|advertisement"
}}

CRITERIA:
‚úÖ HIGH QUALITY (8-10): Complete lessons, detailed explanations, full exercises
‚ùå LOW QUALITY (1-4): Only links, simple answers, advertisements

CONTENT (first 1500 chars):
{content[:1500]}

JSON:"""

            response = self.gemini_model.generate_content(prompt)
            
            if response.text:
                try:
                    json_text = response.text.strip()
                    if json_text.startswith('```json'):
                        json_text = json_text[7:-3]
                    elif json_text.startswith('```'):
                        json_text = json_text[3:-3]
                    
                    analysis = json.loads(json_text.strip())
                    
                    if all(field in analysis for field in ['is_quality', 'score', 'reason']):
                        print(f"   üß† AI Score: {analysis['score']}/10 - {analysis.get('content_type', 'unknown')}")
                        return analysis
                        
                except json.JSONDecodeError:
                    print(f"   ‚ö†Ô∏è AI response parse error")
            
            # Fallback
            return {
                'is_quality': True,
                'score': 6,
                'reason': 'AI analysis failed - using fallback',
                'content_type': 'partial_content'
            }
            
        except Exception as e:
            print(f"   ‚ùå AI analysis error: {e}")
            return {
                'is_quality': True,
                'score': 6,
                'reason': f'AI error: {str(e)[:50]}',
                'content_type': 'unknown'
            }
    
    def process_single_url_two_stage(self, url):
        """X·ª≠ l√Ω m·ªôt URL v·ªõi 2-stage filtering"""
        print(f"\nüîÑ Processing: {url}")
        
        try:
            # STAGE 1: Pre-filtering
            stage1_result = self.stage1_prefilter(url)
            if not stage1_result:
                return None  # Rejected at stage 1
            
            # STAGE 2: AI Analysis
            ai_analysis = self.stage2_ai_analysis(stage1_result['raw_content'], url)
            
            # Combine scores
            combined_score = (stage1_result['prefilter_score'] + ai_analysis['score']) / 2
            
            print(f"   üìä Combined score: {combined_score:.1f}/10 (Pre: {stage1_result['prefilter_score']}, AI: {ai_analysis['score']})")
            
            # Final quality check
            if not ai_analysis['is_quality'] or combined_score < 6:
                reason = f"Low combined score: {combined_score:.1f} - {ai_analysis['reason']}"
                print(f"   üö´ STAGE 2 REJECTED: {reason}")
                self.save_rejected_link(url, reason)
                return None
            
            # STAGE 3: Full content extraction for saving
            print(f"   üìÑ STAGE 3: Full content extraction...")
            html_content = self.get_page_content_advanced(url)
            if not html_content:
                return None
            
            full_content = self.extract_content_advanced(html_content, url)
            if not full_content:
                return None
            
            cleaned_content = self.advanced_content_cleaning(full_content)
            if not cleaned_content or len(cleaned_content) < self.min_content_length:
                return None
            
            print(f"   ‚úÖ TWO-STAGE PASSED: Final content {len(cleaned_content)} chars")
            
            return {
                'url': url,
                'content': cleaned_content,
                'quality_analysis': {
                    'score': round(combined_score, 1),
                    'prefilter_score': stage1_result['prefilter_score'],
                    'ai_score': ai_analysis['score'],
                    'content_type': ai_analysis.get('content_type', 'unknown'),
                    'reason': f"Pre-filter: {stage1_result['prefilter_reason']} | AI: {ai_analysis['reason']}"
                },
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"   ‚ùå Two-stage error: {e}")
            return None
    
    def save_processed_link(self, url):
        """L∆∞u link ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng"""
        self.processed_links.add(url)
        try:
            with open(self.processed_links_file, 'a', encoding='utf-8') as f:
                f.write(f"{url}\n")
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving processed link: {e}")
    
    def save_rejected_link(self, url, reason=""):
        """L∆∞u link b·ªã t·ª´ ch·ªëi ·ªü stage 2"""
        self.rejected_links.add(url)
        try:
            with open(self.rejected_links_file, 'a', encoding='utf-8') as f:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                f.write(f"{url} | {reason} | {timestamp}\n")
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving rejected link: {e}")
    
    def get_links_from_files(self):
        """L·∫•y links ch∆∞a x·ª≠ l√Ω"""
        all_links = []
        
        # Single file
        if self.single_detail_file.exists():
            links_data = self.read_links_from_file(self.single_detail_file)
            if links_data:
                all_links.append({
                    'file_path': self.single_detail_file,
                    'links': links_data
                })
        
        # Multi files  
        if self.multi_detail_links_path.exists():
            txt_files = list(self.multi_detail_links_path.glob("*.txt"))
            for txt_file in txt_files:
                links_data = self.read_links_from_file(txt_file)
                if links_data:
                    all_links.append({
                        'file_path': txt_file,
                        'links': links_data
                    })
        
        return all_links
    
    def read_links_from_file(self, file_path):
        """ƒê·ªçc links ch∆∞a processed/rejected"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                return []
            
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            valid_links = []
            
            for line in lines:
                url = line
                if ' | ' in line:
                    parts = line.split(' | ')
                    url = parts[-1].strip()
                
                if (url.startswith('http') and 
                    not url.lower().endswith('.pdf') and
                    url not in self.processed_links and 
                    url not in self.rejected_links and
                    url not in self.prefilter_rejected):
                    valid_links.append(url)
            
            return valid_links
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading {file_path}: {e}")
            return []
    
    # Include all other necessary methods from previous implementation
    def get_page_content_advanced(self, url):
        """Advanced page loading for stage 3"""
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                if response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                    response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"   ‚ùå Final load error: {e}")
                    return None
                time.sleep(2)
        return None
    
    def extract_content_advanced(self, html_content, url):
        """Advanced content extraction for final stage"""
        try:
            extracted = trafilatura.extract(
                html_content,
                output_format="markdown",
                with_metadata=True,
                include_images=True,
                include_links=False,
                include_tables=True,
                favor_recall=True,
                url=url
            )
            
            if extracted and len(extracted.strip()) > 100:
                return self.clean_extracted_markdown(extracted)
                
            return self.custom_extract_with_bs4(html_content)
            
        except Exception:
            return self.custom_extract_with_bs4(html_content)
    
    def clean_extracted_markdown(self, markdown_text):
        """Clean markdown text"""
        if not markdown_text:
            return markdown_text
        
        lines = markdown_text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            if not line.strip():
                cleaned_lines.append('')
                continue
            
            line = re.sub(r'\[([^\]]+)\]\([^)]*\.html[^)]*\)', r'\1', line)
            line = re.sub(r'\[([^\]]+)\]\(https?://[^)]+\)', r'\1', line)
            line = re.sub(r'https?://[^\s]+\.html\S*', '', line)
            line = re.sub(r'\s+', ' ', line).strip()
            
            if line and len(line) > 3:
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines)
        while '\n\n\n' in result:
            result = result.replace('\n\n\n', '\n\n')
        
        return result.strip()
    
    def custom_extract_with_bs4(self, html_content):
        """BeautifulSoup extraction fallback"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            for unwanted in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                unwanted.decompose()
            
            content_selectors = [
                '.entry-content', '.post-content', '.content',
                'article', 'main', '.main-content', '.post-body'
            ]
            
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element and len(element.get_text().strip()) > 200:
                    markdown_content = md(
                        str(element),
                        heading_style="ATX",
                        bullets="*",
                        strip=['script', 'style'],
                        convert=['img'],
                        escape_misc=False
                    )
                    return self.clean_extracted_markdown(markdown_content)
            
            return None
            
        except Exception:
            return None
    
    def advanced_content_cleaning(self, text):
        """Advanced content cleaning"""
        if not text:
            return text
        
        lines = text.split('\n')
        cleaned_lines = []
        
        skip_patterns = [
            'ch·ªâ t·ª´', 'mua tr·ªçn b·ªô', 'g·ª≠i ph√≠', 'tk:', 't√†i kho·∫£n',
            'ng√¢n h√†ng', 'chuy·ªÉn kho·∫£n', 'thanh to√°n',
            'v·ªõi b·ªô', 's·∫Ω gi√∫p h·ªçc sinh', 'ƒë∆∞·ª£c bi√™n so·∫°n',
            'xin g·ª≠i t·ªõi', 'm·ªùi c√°c b·∫°n', 'download', 't·∫£i v·ªÅ',
            'xem th√™m', 'li√™n h·ªá', 'facebook', 'zalo',
            'ƒë√°p √°n: a', 'ƒë√°p √°n: b', 'ƒë√°p √°n: c', 'ƒë√°p √°n: d'
        ]
        
        for line in lines:
            line_lower = line.lower().strip()
            
            if len(line_lower) < 3:
                if line.strip() == '':
                    cleaned_lines.append('')
                continue
            
            skip = any(pattern in line_lower for pattern in skip_patterns)
            
            if not skip and line.strip():
                digit_count = sum(c.isdigit() for c in line)
                if digit_count > len(line) * 0.4 and len(line.strip()) < 50:
                    skip = True
            
            if not skip and line.strip():
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines)
        while '\n\n\n' in result:
            result = result.replace('\n\n\n', '\n\n')
        
        return result.strip()
    
    def save_content_to_md_enhanced(self, content_data):
        """L∆∞u content v·ªõi metadata chi ti·∫øt"""
        try:
            url = content_data['url']
            domain = url.split('/')[2].replace('www.', '')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            quality_score = content_data['quality_analysis']['score']
            safe_url = re.sub(r'[^\w\-.]', '_', url.split('/')[-1])[:40]
            filename = f"{domain}_s{quality_score}_{safe_url}_{timestamp}.md"
            
            filepath = self.output_path / filename
            
            # Enhanced markdown v·ªõi 2-stage info
            md_content = f"""# {url}

**Scraped at:** {content_data['timestamp']}
**Source:** {url}
**Combined Score:** {quality_score}/10
**Pre-filter Score:** {content_data['quality_analysis']['prefilter_score']}/8
**AI Score:** {content_data['quality_analysis']['ai_score']}/10
**Content Type:** {content_data['quality_analysis']['content_type']}

---

{content_data['content']}

---

*Two-Stage Analysis: {content_data['quality_analysis']['reason']}*
"""
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            print(f"üíæ Saved: {filename}")
            return filepath
            
        except Exception as e:
            print(f"‚ùå Error saving content: {e}")
            return None
    
    def update_file_remove_link(self, file_path, processed_url):
        """X√≥a link ƒë√£ x·ª≠ l√Ω kh·ªèi file g·ªëc"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            updated_lines = []
            for line in lines:
                if processed_url not in line.strip():
                    updated_lines.append(line)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(updated_lines)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating file {file_path}: {e}")
    
    def process_all_links_two_stage(self, max_links_per_session=200):
        """X·ª≠ l√Ω links v·ªõi 2-stage filtering system"""
        print("\nüöÄ STARTING TWO-STAGE INTELLIGENT CONTENT SCRAPING")
        print("=" * 70)
        
        # Configuration display
        print(f"üîç STAGE 1 - Pre-filtering:")
        print(f"   ‚Ä¢ Min content length: {self.prefilter_config['min_raw_content_length']} chars")
        print(f"   ‚Ä¢ Min text lines: {self.prefilter_config['min_text_lines']}")
        print(f"   ‚Ä¢ Max link density: {self.prefilter_config['max_link_density']*100}%")
        print(f"   ‚Ä¢ Required edu keywords: {len(self.prefilter_config['required_educational_keywords'])}")
        
        print(f"\nü§ñ STAGE 2 - AI Analysis:")
        print(f"   ‚Ä¢ Model: {'‚úÖ ' + (self.gemini_model._model_name if self.gemini_model else 'None')}")
        print(f"   ‚Ä¢ Min AI score: {self.min_educational_score}/10")
        
        print(f"\nüéØ Processing Settings:")
        print(f"   ‚Ä¢ Max links per session: {max_links_per_session}")
        print(f"   ‚Ä¢ Output: {self.output_path}")
        
        # Get links
        link_files = self.get_links_from_files()
        if not link_files:
            print("\nüìÇ No new links to process")
            return
        
        # Flatten and limit links
        all_links_flat = []
        for file_data in link_files:
            for url in file_data['links']:
                all_links_flat.append({
                    'url': url,
                    'file_path': file_data['file_path']
                })
        
        total_available = len(all_links_flat)
        
        # Apply session limit
        if total_available > max_links_per_session:
            print(f"\n‚ö†Ô∏è SESSION LIMITING ENABLED")
            print(f"   üìä Total available: {total_available:,}")
            print(f"   üéØ Processing: {max_links_per_session}")
            print(f"   ‚è≠Ô∏è Remaining after: {total_available - max_links_per_session:,}")
            
            links_to_process = all_links_flat[:max_links_per_session]
            
            # Estimate filtering
            print(f"\nüìà EXPECTED FILTERING:")
            print(f"   üîç Stage 1 (pre-filter): ~70-80% rejection")
            print(f"   ü§ñ Stage 2 (AI): ~30-50% rejection of passed")
            print(f"   ‚úÖ Final success: ~10-20% of total ({max_links_per_session * 0.15:.0f} links)")
            
            confirm = input(f"\n‚ùì Process {max_links_per_session} links now? (y/N): ").strip().lower()
            if confirm != 'y':
                print("üõë Processing cancelled")
                return
        else:
            links_to_process = all_links_flat
        
        total_links = len(links_to_process)
        print(f"\nüìÑ Processing {total_links} links")
        
        # Processing stats
        processed_count = 0
        stage1_passed = 0
        stage2_passed = 0
        final_success = 0
        stage1_rejected = 0
        stage2_rejected = 0
        errors = 0
        
        # Time estimation
        estimated_seconds = total_links * 3  # 3 gi√¢y/link trung b√¨nh v·ªõi pre-filter
        print(f"‚è±Ô∏è Estimated time: {estimated_seconds/60:.0f} minutes")
        
        start_time = time.time()
        
        # Process each link
        for idx, link_data in enumerate(links_to_process, 1):
            try:
                url = link_data['url']
                file_path = link_data['file_path']
                
                print(f"\n{'='*80}")
                print(f"üîç [{idx}/{total_links}] Processing...")
                print(f"üåê URL: {url}")
                
                # Two-stage processing
                content_data = self.process_single_url_two_stage(url)
                
                if content_data:
                    # Successful processing
                    saved_file = self.save_content_to_md_enhanced(content_data)
                    if saved_file:
                        self.save_processed_link(url)
                        self.update_file_remove_link(file_path, url)
                        final_success += 1
                        stage1_passed += 1
                        stage2_passed += 1
                        
                        print(f"‚úÖ FINAL SUCCESS: High-quality content saved")
                        print(f"   üìä Combined Score: {content_data['quality_analysis']['score']}/10")
                        print(f"   üìÑ Content Length: {len(content_data['content'])}")
                        print(f"   üíæ File: {saved_file.name}")
                    else:
                        errors += 1
                        print(f"‚ùå Save error")
                else:
                    # Check which stage rejected it
                    if url in self.prefilter_rejected:
                        stage1_rejected += 1
                        print(f"üö´ STAGE 1 REJECTED")
                    elif url in self.rejected_links:
                        stage1_passed += 1
                        stage2_rejected += 1
                        print(f"üö´ STAGE 2 REJECTED")
                    else:
                        errors += 1
                        print(f"‚ùå PROCESSING ERROR")
                
                processed_count += 1
                
                # Progress report every 10 links
                if processed_count % 10 == 0 or processed_count == total_links:
                    elapsed = time.time() - start_time
                    progress = (processed_count / total_links) * 100
                    avg_time = elapsed / processed_count
                    remaining_time = (total_links - processed_count) * avg_time
                    
                    print(f"\nüìà PROGRESS REPORT [{processed_count}/{total_links}] ({progress:.1f}%)")
                    print(f"   üîç Stage 1 passed: {stage1_passed} ({stage1_passed/processed_count*100:.1f}%)")
                    print(f"   ü§ñ Stage 2 passed: {stage2_passed} ({stage2_passed/max(stage1_passed,1)*100:.1f}%)")
                    print(f"   ‚úÖ Final success: {final_success} ({final_success/processed_count*100:.1f}%)")
                    print(f"   ‚è±Ô∏è Avg time/link: {avg_time:.1f}s | ETA: {remaining_time/60:.0f}min")
                
                # Adaptive delay based on success rate
                if processed_count >= 20:
                    success_rate = final_success / processed_count
                    if success_rate > 0.3:  # High success rate - can speed up
                        delay = 1
                    elif success_rate > 0.15:  # Normal success rate
                        delay = 2
                    else:  # Low success rate - slow down for quality
                        delay = 3
                else:
                    delay = 2  # Default delay
                
                if processed_count < total_links:
                    print(f"‚è≥ Waiting {delay}s...")
                    time.sleep(delay)
                
            except KeyboardInterrupt:
                print(f"\nüõë Processing interrupted by user")
                break
            except Exception as e:
                print(f"‚ùå Unexpected error: {e}")
                processed_count += 1
                errors += 1
                continue
        
        # Final comprehensive summary
        total_time = time.time() - start_time
        
        print(f"\n{'='*80}")
        print(f"üéâ TWO-STAGE PROCESSING COMPLETE!")
        print(f"{'='*80}")
        
        print(f"üìä DETAILED STATISTICS:")
        print(f"   üìÑ Total processed: {processed_count}")
        print(f"   ‚è±Ô∏è Total time: {total_time/60:.1f} minutes")
        print(f"   üöÄ Avg speed: {total_time/processed_count:.1f} seconds/link")
        
        print(f"\nüîç STAGE 1 (Pre-filtering):")
        print(f"   ‚úÖ Passed: {stage1_passed} ({stage1_passed/processed_count*100:.1f}%)")
        print(f"   üö´ Rejected: {stage1_rejected} ({stage1_rejected/processed_count*100:.1f}%)")
        
        print(f"\nü§ñ STAGE 2 (AI Analysis):")
        if stage1_passed > 0:
            print(f"   ‚úÖ Passed: {stage2_passed} ({stage2_passed/stage1_passed*100:.1f}%)")
            print(f"   üö´ Rejected: {stage2_rejected} ({stage2_rejected/stage1_passed*100:.1f}%)")
        else:
            print(f"   ‚úÖ Passed: {stage2_passed}")
            print(f"   üö´ Rejected: {stage2_rejected}")
        
        print(f"\nüéØ FINAL RESULTS:")
        print(f"   ‚úÖ Successfully saved: {final_success} ({final_success/processed_count*100:.1f}%)")
        print(f"   ‚ùå Errors: {errors} ({errors/processed_count*100:.1f}%)")
        
        print(f"\nüìÅ OUTPUT LOCATIONS:")
        print(f"   üíæ Content: {self.output_path}")
        print(f"   üîç Pre-filter rejected: {self.prefilter_rejected_file}")
        print(f"   ü§ñ AI rejected: {self.rejected_links_file}")
        print(f"   ‚úÖ Processed: {self.processed_links_file}")
        
        # Efficiency analysis
        if processed_count > 0:
            efficiency = final_success / processed_count
            if efficiency > 0.2:
                print(f"\nüéä EXCELLENT EFFICIENCY: {efficiency*100:.1f}% success rate!")
            elif efficiency > 0.1:
                print(f"\nüëç GOOD EFFICIENCY: {efficiency*100:.1f}% success rate")
            else:
                print(f"\nüîß LOW EFFICIENCY: {efficiency*100:.1f}% - consider adjusting filters")
        
        # Remaining work estimate
        remaining = total_available - processed_count
        if remaining > 0:
            remaining_time_est = remaining * (total_time / processed_count) / 3600  # hours
            print(f"\n‚è≠Ô∏è REMAINING WORK:")
            print(f"   üìä Links left: {remaining:,}")
            print(f"   ‚è±Ô∏è Est. time: {remaining_time_est:.1f} hours")
            print(f"   üí° Tip: Run again with max_links_per_session={max_links_per_session}")
    
    def get_statistics(self):
        """L·∫•y th·ªëng k√™ chi ti·∫øt cho c·∫£ 2 stages"""
        stats = {
            'processed_links': len(self.processed_links),
            'prefilter_rejected': len(self.prefilter_rejected),
            'ai_rejected': len(self.rejected_links),
            'total_handled': len(self.processed_links) + len(self.prefilter_rejected) + len(self.rejected_links)
        }
        
        # Calculate filtering efficiency
        if stats['total_handled'] > 0:
            stats['stage1_pass_rate'] = (len(self.processed_links) + len(self.rejected_links)) / stats['total_handled']
            stats['final_success_rate'] = len(self.processed_links) / stats['total_handled']
        
        return stats
    
    def print_statistics(self):
        """In th·ªëng k√™ chi ti·∫øt"""
        stats = self.get_statistics()
        
        print(f"\nüìä TWO-STAGE SCRAPER STATISTICS")
        print(f"{'='*60}")
        print(f"‚úÖ Successfully processed: {stats['processed_links']}")
        print(f"üîç Pre-filter rejected: {stats['prefilter_rejected']}")
        print(f"ü§ñ AI rejected: {stats['ai_rejected']}")
        print(f"üìÑ Total handled: {stats['total_handled']}")
        
        if stats['total_handled'] > 0:
            print(f"\nüìà EFFICIENCY METRICS:")
            print(f"   üîç Stage 1 pass rate: {stats.get('stage1_pass_rate', 0)*100:.1f}%")
            print(f"   üéØ Final success rate: {stats.get('final_success_rate', 0)*100:.1f}%")
            
            # Filtering effectiveness
            total_rejected = stats['prefilter_rejected'] + stats['ai_rejected']
            print(f"   üõ°Ô∏è Total filtered out: {total_rejected} ({total_rejected/stats['total_handled']*100:.1f}%)")


# Backward compatibility
IntegratedContentScraper = TwoStageContentScraper

def main():
    """Test function v·ªõi two-stage processing"""
    print("üß™ TESTING TWO-STAGE CONTENT SCRAPER")
    print("=" * 50)
    
    scraper = TwoStageContentScraper()
    
    # Show current statistics
    scraper.print_statistics()
    
    # Debug file existence
    print(f"\nüîç File existence check:")
    print(f"   Single file exists: {scraper.single_detail_file.exists()}")
    print(f"   Multi folder exists: {scraper.multi_detail_links_path.exists()}")
    
    if scraper.multi_detail_links_path.exists():
        txt_files = list(scraper.multi_detail_links_path.glob("*.txt"))
        print(f"   Files in multi folder: {[f.name for f in txt_files]}")
    
    # Get link count
    link_files = scraper.get_links_from_files()
    total_new_links = sum(len(file_data['links']) for file_data in link_files)
    
    if total_new_links > 0:
        print(f"\nüìÑ Found {total_new_links:,} NEW links to process")
        
        # Suggest batch size
        if total_new_links > 1000:
            suggested_batch = min(500, total_new_links)
            print(f"üí° Suggested batch size: {suggested_batch}")
        else:
            suggested_batch = total_new_links
        
        batch_size = input(f"üéØ Enter batch size (default {suggested_batch}): ").strip()
        if not batch_size:
            batch_size = suggested_batch
        else:
            batch_size = int(batch_size)
        
        confirm = input(f"‚ùì Start two-stage processing for {batch_size} links? (y/N): ").strip().lower()
        if confirm == 'y':
            scraper.process_all_links_two_stage(max_links_per_session=batch_size)
        else:
            print("üõë Processing cancelled")
    else:
        print("\n‚úÖ No new links to process")


if __name__ == "__main__":
    main()