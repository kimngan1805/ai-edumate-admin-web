# ===== app/web_scraping/hocmai_crawler.py (MODIFIED: ONLY CRAWL & MOVE) =====
"""
HocMai crawler: Ch·ªâ crawl v√† t·∫°o PDF, sau ƒë√≥ chuy·ªÉn sang folder cho BackgroundPDFProcessor x·ª≠ l√Ω.
Kh√¥ng c√≤n x·ª≠ l√Ω metadata v√† upload MinIO tr·ª±c ti·∫øp trong file n√†y.
"""
import requests
from bs4 import BeautifulSoup
import os
import img2pdf
from urllib.parse import urljoin
import time
from pathlib import Path
import json
import logging
import shutil # Import shutil ƒë·ªÉ di chuy·ªÉn file

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# KH√îNG C√íN IMPORT PDFMetadataProcessor ·ªû ƒê√ÇY N·ªÆA
# (V√¨ vi·ªác x·ª≠ l√Ω metadata ƒë√£ ƒë∆∞·ª£c chuy·ªÉn sang BackgroundPDFProcessor trong run.py)

class HocMaiCrawler:
    def __init__(self, base_output_dir: Path = None): # type: ignore # Kh√¥ng c·∫ßn gemini_api_key, minio_config ·ªü ƒë√¢y n·ªØa
        # Th∆∞ m·ª•c g·ªëc cho t·∫•t c·∫£ c√°c output c·ªßa HocMai Crawler
        if base_output_dir is None:
            try:
                from .config import MULTI_DETAIL_LINK_DIR
            except ImportError:
                logger.warning("‚ùå config.py not found or MULTI_DETAIL_LINK_DIR not defined. Using default 'crawled_output'.")
                MULTI_DETAIL_LINK_DIR = Path("crawled_output") 
                
            self.base_output_dir = MULTI_DETAIL_LINK_DIR / "hocmai_crawls" 
        else:
            self.base_output_dir = base_output_dir
            
        os.makedirs(self.base_output_dir, exist_ok=True)
        
        # Th∆∞ m·ª•c TRUNG GIAN ƒë·ªÉ l∆∞u PDF T·∫†M TH·ªúI sau khi crawl
        # Sau ƒë√≥ s·∫Ω ƒë∆∞·ª£c chuy·ªÉn sang downloaded_pdfs
        self.temp_pdf_dir = self.base_output_dir / "pdfs_from_hocmai"
        os.makedirs(self.temp_pdf_dir, exist_ok=True) 
        
        # Th∆∞ m·ª•c ƒë√≠ch cu·ªëi c√πng cho PDF, n∆°i BackgroundPDFProcessor s·∫Ω gi√°m s√°t
        self.final_pdf_destination_dir = Path("downloaded_pdfs")
        os.makedirs(self.final_pdf_destination_dir, exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.current_crawl_output_dir = self.base_output_dir / f"crawl_session_{timestamp}"
        
        self.save_images = self.current_crawl_output_dir / "images"
        
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        os.makedirs(self.current_crawl_output_dir, exist_ok=True)
        os.makedirs(self.save_images, exist_ok=True)
        
        logger.info(f"üìÅ Base output directory for reports/images: {self.base_output_dir}")
        logger.info(f"üìÅ Temporary PDF directory: {self.temp_pdf_dir}")
        logger.info(f"üìÅ Final PDF destination for processing: {self.final_pdf_destination_dir}")

        # KH√îNG C√íN KH·ªûI T·∫†O PDFMetadataProcessor ·ªû ƒê√ÇY N·ªÆA
        self.pdf_processor = None # ƒê·∫£m b·∫£o l√† None ƒë·ªÉ kh√¥ng c√≥ l·ªói g·ªçi
        logger.info("‚ÑπÔ∏è PDFMetadataProcessor initialization is now handled by BackgroundPDFProcessor in run.py.")


    def get_all_links(self, base_url=None):
        """
        L·∫•y danh s√°ch link b√†i h·ªçc t·ª´ trang ch·ªß, c√°c danh m·ª•c v√† c√°c trang ph√¢n trang.
        S·ª≠ d·ª•ng BFS ƒë·ªÉ duy·ªát c√°c link m·ªôt c√°ch ch·ªß ƒë·ªông v√† s√¢u h∆°n.
        """
        initial_url = base_url or "https://hocmai.vn/kho-tai-lieu/"
        logger.info(f"üîç ƒêang l·∫•y links t·ª´: {initial_url}")
        
        all_lesson_links = set() # D√πng set ƒë·ªÉ tr√°nh tr√πng l·∫∑p ((title, url))
        visited_urls = set()
        urls_to_visit = [initial_url]
        
        MAX_PAGES_TO_CRAWL = 50 # Gi·ªõi h·∫°n trang ƒë·ªÉ c√†o s√¢u, c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh
        pages_crawled = 0

        while urls_to_visit and pages_crawled < MAX_PAGES_TO_CRAWL:
            current_url = urls_to_visit.pop(0)
            if current_url in visited_urls:
                continue
            
            logger.info(f"  ƒêang thƒÉm trang: {current_url} (Trang {pages_crawled+1}/{MAX_PAGES_TO_CRAWL})")
            visited_urls.add(current_url)
            pages_crawled += 1

            try:
                r = requests.get(current_url, headers=self.headers, timeout=15) 
                r.raise_for_status() 
                soup = BeautifulSoup(r.text, "html.parser")
                
                # B∆∞·ªõc 1: T√¨m c√°c link b√†i h·ªçc (c√≥ 'read.php?id=') tr√™n trang hi·ªán t·∫°i
                for a in soup.find_all("a", href=True):
                    href = a["href"]  # type: ignore
                    full_link = urljoin(current_url, href)  # type: ignore

                    if "read.php?id=" in full_link:
                        title = a.get_text(strip=True)
                        if title and full_link not in [item[1] for item in all_lesson_links]: 
                            all_lesson_links.add((title, full_link))
                            
                    # B∆∞·ªõc 2: T√¨m c√°c link danh m·ª•c v√† link ph√¢n trang ƒë·ªÉ th√™m v√†o h√†ng ƒë·ª£i duy·ªát
                    if "/kho-tai-lieu/" in full_link and \
                       ("lop-" in full_link or "mon-hoc/" in full_link or \
                        "de-thi-" in full_link or "thi-vao-lop-" in full_link or \
                        "/page-" in full_link) and \
                       "read.php?id=" not in full_link: 
                        
                        if full_link not in visited_urls and full_link not in urls_to_visit:
                            urls_to_visit.append(full_link)
                            logger.debug(f"    T√¨m th·∫•y danh m·ª•c/ph√¢n trang m·ªõi: {full_link}")
                            
            except requests.exceptions.Timeout:
                logger.warning(f"  ‚ö†Ô∏è Timeout khi l·∫•y links t·ª´: {current_url}")
            except requests.exceptions.RequestException as e:
                logger.error(f"  ‚ùå L·ªói m·∫°ng ho·∫∑c HTTP khi l·∫•y links t·ª´ {current_url}: {e}")
            except Exception as e:
                logger.error(f"  ‚ùå L·ªói khi ph√¢n t√≠ch HTML ho·∫∑c l·∫•y links t·ª´ {current_url}: {e}")
            
            time.sleep(1) # Ngh·ªâ gi·ªØa c√°c request l·∫•y link ƒë·ªÉ tr√°nh b·ªã ch·∫∑n

        logger.info(f"‚úÖ T√¨m th·∫•y {len(all_lesson_links)} b√†i h·ªçc (sau khi duy·ªát {pages_crawled} trang)")
        return list(all_lesson_links)

    def download_images_and_make_pdf(self, title, url):
        """T·∫£i ·∫£nh v√† t·∫°o PDF - l∆∞u v√†o th∆∞ m·ª•c T·∫†M TH·ªúI"""
        logger.info(f"üì• ƒêang x·ª≠ l√Ω: {title}")

        try:
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_title = safe_title.replace(" ", "_")[:100]
            
            if not safe_title:
                safe_title = f"untitled_{int(time.time())}"

            folder_name_for_images = self.save_images / safe_title 
            os.makedirs(folder_name_for_images, exist_ok=True)

            r = requests.get(url, headers=self.headers, timeout=20) 
            r.raise_for_status() 
            soup = BeautifulSoup(r.text, "html.parser")
            carousel = soup.find("div", id="khotailieu")
            imgs = carousel.find_all("img") if carousel else []  # type: ignore

            img_urls = []
            for img in imgs:
                src = img.get("src") or img.get("data-src") # type: ignore
                if src and "/documents/" in src: 
                    img_urls.append(src)

            if not img_urls:
                logger.warning(f"‚ùå No images found for {title} at {url}")
                return {"status": "error", "message": "No images found", "pdf_path": None}

            logger.info(f"üì∏ T√¨m th·∫•y {len(img_urls)} ·∫£nh cho {title}")

            image_paths = []
            for i, img_url in enumerate(img_urls, start=1):
                try:
                    full_img_url = urljoin(url, img_url)
                    img_data = requests.get(full_img_url, headers=self.headers, timeout=15).content 
                    img_path = folder_name_for_images / f"page_{i}.png" 
                    
                    with open(img_path, "wb") as f:
                        f.write(img_data)
                    image_paths.append(str(img_path))
                except requests.exceptions.Timeout:
                    logger.warning(f"  ‚ö†Ô∏è Timeout khi t·∫£i ·∫£nh {i} t·ª´: {full_img_url}") # type: ignore
                except requests.exceptions.RequestException as e:
                    logger.warning(f"  ‚ö†Ô∏è L·ªói t·∫£i ·∫£nh {i} (m·∫°ng/HTTP): {e}")
                except Exception as e:
                    logger.warning(f"  ‚ö†Ô∏è L·ªói t·∫£i ·∫£nh {i}: {e}")

            if not image_paths:
                logger.warning(f"‚ùå Kh√¥ng c√≥ ·∫£nh n√†o ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng cho {title}")
                return {"status": "error", "message": "No images downloaded successfully", "pdf_path": None}

            # L∆∞u PDF v√†o th∆∞ m·ª•c T·∫†M TH·ªúI (self.temp_pdf_dir)
            pdf_path = self.temp_pdf_dir / f"{safe_title}.pdf"
            try:
                with open(pdf_path, "wb") as f:
                    if image_paths:
                        f.write(img2pdf.convert(image_paths))
                    else:
                        raise ValueError("No images to convert to PDF.")
                
                logger.info(f"‚úÖ PDF created: {pdf_path}")
                
                return {
                    "status": "success",
                    "title": title,
                    "images_count": len(image_paths),
                    "pdf_path": str(pdf_path) # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n PDF t·∫°m th·ªùi
                }
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·∫°o PDF t·ª´ ·∫£nh cho {title}: {e}")
                return {"status": "error", "message": f"Failed to create PDF: {e}", "pdf_path": None}
            
        except requests.exceptions.Timeout:
            logger.error(f"‚ùå Timeout khi x·ª≠ l√Ω {title}: {url}")
            return {"status": "error", "message": "Timeout during processing", "pdf_path": None}
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå L·ªói m·∫°ng ho·∫∑c HTTP khi x·ª≠ l√Ω {title}: {e}")
            return {"status": "error", "message": str(e), "pdf_path": None}
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω {title}: {e}")
            return {"status": "error", "message": str(e), "pdf_path": None}

    def crawl_hocmai(self, base_url=None, max_lessons=None):
        """Main function - Ch·ªâ crawl v√† t·∫°o PDF, sau ƒë√≥ chuy·ªÉn file"""
        logger.info("üöÄ B·∫Øt ƒë·∫ßu crawl HocMai...")
        
        links = self.get_all_links(base_url)
        if not links:
            logger.info("‚ùå Kh√¥ng t√¨m th·∫•y b√†i h·ªçc n√†o ƒë·ªÉ c√†o.")
            return {"status": "error", "message": "Kh√¥ng t√¨m th·∫•y b√†i h·ªçc n√†o", 
                    "total_lessons_crawled": 0, "successful_downloads": 0, "failed_downloads": 0,
                    "minio_upload_summary": {"status": "skipped", "reason": "No PDFs processed in this module"}}
        
        if max_lessons:
            links = links[:max_lessons]
        
        logger.info(f"Found {len(links)} lessons to process (potentially limited by max_lessons)")
        
        results = []
        success_count = 0
        
        # Danh s√°ch c√°c file PDF ƒë√£ t·∫°o th√†nh c√¥ng trong phi√™n n√†y
        created_pdf_paths = [] 

        for i, (title, link) in enumerate(links, 1):
            logger.info(f"\n[{i}/{len(links)}] Processing: {title} - {link}")
            
            result = self.download_images_and_make_pdf(title, link)
            result["lesson_number"] = i
            result["url"] = link
            
            if result["status"] == "success":
                success_count += 1
                created_pdf_paths.append(Path(result["pdf_path"])) # Th√™m v√†o danh s√°ch ƒë·ªÉ di chuy·ªÉn sau
            
            results.append(result)
            
            time.sleep(1) # Ngh·ªâ gi·ªØa c√°c request t·∫£i b√†i h·ªçc
        
        # --- Di chuy·ªÉn c√°c file PDF ƒë√£ t·∫°o sang th∆∞ m·ª•c ƒë√≠ch cu·ªëi c√πng ---
        logger.info("\n" + "=" * 60)
        logger.info(f"üì¶ ƒêang di chuy·ªÉn c√°c file PDF t·ª´ {self.temp_pdf_dir} sang {self.final_pdf_destination_dir}...")
        moved_count = 0
        for pdf_file_path in created_pdf_paths:
            try:
                if pdf_file_path.exists():
                    shutil.move(str(pdf_file_path), str(self.final_pdf_destination_dir / pdf_file_path.name))
                    moved_count += 1
                    logger.info(f"  ‚úÖ ƒê√£ di chuy·ªÉn: {pdf_file_path.name}")
                else:
                    logger.warning(f"  ‚ö†Ô∏è File kh√¥ng t·ªìn t·∫°i ƒë·ªÉ di chuy·ªÉn: {pdf_file_path.name}")
            except Exception as e:
                logger.error(f"  ‚ùå L·ªói khi di chuy·ªÉn {pdf_file_path.name}: {e}")
        logger.info(f"‚úÖ ƒê√£ di chuy·ªÉn {moved_count}/{len(created_pdf_paths)} file PDF.")
        logger.info("=" * 60)

        # C·∫≠p nh·∫≠t summary ƒë·ªÉ ph·∫£n √°nh vi·ªác kh√¥ng x·ª≠ l√Ω metadata ·ªü ƒë√¢y
        summary = {
            "status": "completed",
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_lessons_crawled": len(links), 
            "successful_downloads": success_count, 
            "failed_downloads": len(links) - success_count,
            "moved_pdfs_count": moved_count,
            "download_results": results,
            "download_folders": {
                "base_output_dir": str(self.base_output_dir), 
                "temporary_pdfs_dir": str(self.temp_pdf_dir), # ƒê·ªïi t√™n th√†nh t·∫°m th·ªùi
                "final_pdfs_destination_dir": str(self.final_pdf_destination_dir), # Th√™m th∆∞ m·ª•c ƒë√≠ch
                "current_session_images_dir": str(self.save_images) 
            },
            "minio_upload_summary": {"status": "delegated", "reason": "Processing handled by BackgroundPDFProcessor in run.py"} 
        }
        
        report_file = self.current_crawl_output_dir / f"hocmai_report_{int(time.time())}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nüéâ Ho√†n th√†nh! {success_count}/{len(links)} b√†i h·ªçc th√†nh c√¥ng")
        logger.info(f"üìä B√°o c√°o: {report_file}")
        
        return summary

# ===== MAIN SCRIPT TO RUN CRAWLER (V√≠ d·ª• c√°ch g·ªçi) =====
if __name__ == "__main__":
    # L∆∞u √Ω: Khi g·ªçi t·ª´ api.py, c√°c tham s·ªë n√†y s·∫Ω ƒë∆∞·ª£c truy·ªÅn t·ª´ ƒë√≥.
    # Khi ch·∫°y file n√†y tr·ª±c ti·∫øp, c√°c tham s·ªë n√†y s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng.
    # Kh√¥ng c·∫ßn Gemini API key ho·∫∑c MinIO config ·ªü ƒë√¢y n·ªØa.
    
    BASE_CRAWLER_OUTPUT_DIR = Path("hocmai_output_all") 

    crawler = HocMaiCrawler(
        base_output_dir=BASE_CRAWLER_OUTPUT_DIR
    )
    
    crawl_summary = crawler.crawl_hocmai(max_lessons=None) 

    logger.info("\n--- T√≥m t·∫Øt cu·ªëi c√πng ---")
    logger.info(json.dumps(crawl_summary, indent=2, ensure_ascii=False))